main location
'/mnt/c/Users/Administrator/Documents/finalProject'

when actually doing python 
run.py --config configs/mi.json

for ubuntu, set environment variables with
export MIMIR_CACHE_PATH='/mnt/c/Users/Administrator/Documents/finalProject/cache'
export MIMIR_DATA_SOURCE='/mnt/c/Users/Administrator/Documents/finalProject/data'


run.py notes
- texts (around line 111 in run.py) actually contains the dataset in batches I think
- results is a list of dictionaries which contain a string (sample) and the associated scores it gets for each test
- I'm getting close....
- predictions is a dictionary that contains lists of scores (loss, zlib, etc) associated with particular attacks
    - Now, I need to figure out what is done with those scores to determine the success of the attacks (do they sort like the og paper?)
    - samples is the associated samples I think
    - it splits member/nonmember by the train/test files in the cache
- I feel like I'm less close   
- Not sure exactly how member/non-member is used in run
- data metrics are done by using member/nonmember scores somehow, with ROC curve....
- IDK HOW TO LOAD CUSTOM DATASETS THE README SAYS USE LOAD IN DATA UTILS BUT HOW
FOR TEXTCOMPLEXITY

go to textcomplexity folder
to analyze, files must be in connl-u format
    to do this, put the input document file in the input folder
    go to the utils folder and run
    python3 run_stanza.py ../input/arxiv_ngram_7_0.2.jsonl -l english -o ../output
    this will create the conll-u file in the output folder
now just run the following line with the conll-u file to print info about the text 
txtcomplexity --input-format conllu arxiv_ngram_7_0.2.jsonl.conllu